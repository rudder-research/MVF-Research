# VCF Research**
**VCF Research â€“ Vector Coherence Framework**

A large-scale, cloud-based research engine for studying macro-financial behavior through geometric, statistical, and structural alignment.


**ğŸ“Œ Overview**

----------------------------------------------------------

**VCF Research is an open computational framework designed to:**
  -ingest large-scale macroeconomic and financial datasets
  -normalize heterogeneous data into a unified statistical space
  -construct monthly and daily macro-risk panels
  -analyze vector relationships among economic signals
  -compute geometric indicators including Î¸ (theta), Ï† (phi), coherence, and divergence
  -run automated workflows via GitHub Actions
  -enable reproducible, transparent, academically-viable macro modeling

----------------------------------------------------------

The goal is not to forecast markets or make trading decisions.
The goal is to understand how macro forces behave â€”
how they align, diverge, resonate, and evolve across time â€”
and whether this structure reveals stable, measurable patterns.

----------------------------------------------------------

**VCF is built to be:**
  -cloud-native
  -reproducible
  -modular
  -extensible
  -mathematically clean
  -academically publishable*

----------------------------------------------------------------------

**ğŸ— Project Architecture**

```
VCF_Research/
â”‚
â”œâ”€â”€ notebooks/               # Jupyter notebooks for research and analysis
â”‚   â”œâ”€â”€ VFC_Mathematical_Engine_Claud.ipynb
â”‚   â”œâ”€â”€ Visualization_Suite_Claud.ipynb
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ src/                     # Reusable Python modules and libraries
â”‚   â”œâ”€â”€ vcf_advanced_math.py
â”‚   â”œâ”€â”€ vcf_visualizations.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ scripts/                 # ETL, normalization, panel construction
â”‚   â”œâ”€â”€ data_loader.py
â”‚   â”œâ”€â”€ normalize_metrics.py
â”‚   â”œâ”€â”€ build_macro_panel.py
â”‚   â”œâ”€â”€ geometry_engine.py
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data_raw/                # Unprocessed market + macro data (Colab writes)
â”‚   â”œâ”€â”€ SPY_US.csv
â”‚   â”œâ”€â”€ GDP_US.csv
â”‚   â”œâ”€â”€ CPI_US.csv
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ data_clean/              # Normalized series (Colab writes)
â”‚   â”œâ”€â”€ *_normalized.csv
â”‚   â”œâ”€â”€ macro_monthly_panel.csv
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ geometry/                # GitHub-run analysis engine (Î¸, Ï†, coherence)
â”‚   â”œâ”€â”€ geometry_panel.csv
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ registry/                # Metric definitions (JSON, CSV)
â”‚   â”œâ”€â”€ vcf_metric_registry.json
â”‚   â”œâ”€â”€ metrics.csv
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ docs/                    # Project documentation
â”‚   â”œâ”€â”€ DATA_SOURCES.md      # Data source and preprocessing information
â”‚   â”œâ”€â”€ log.md               # Experiment and process notes
â”‚   â””â”€â”€ *.md                 # Additional research documentation
â”‚
â”œâ”€â”€ assets/                  # Static files, archives, and references
â”‚   â”œâ”€â”€ *.zip
â”‚   â”œâ”€â”€ *.pdf
â”‚   â””â”€â”€ *.gdoc
â”‚
â”œâ”€â”€ .github/workflows/       # CI/CD automation
â”‚   â””â”€â”€ run_geometry.yml
â”‚
â”œâ”€â”€ .gitignore               # Exclude large files and build artifacts
â””â”€â”€ README.md                # This file
```


----------------------------------------------------------

**ğŸš€ Two-Level Compute Architecture**

**1. Colab Notebook (Data Engine)**
Handles all data operations:
  -FRED / Yahoo ingestion
  -normalization
  -monthly panel construction
  -trimming to full-coverage windows
  -pushing all generated files to GitHub

This isolates data work in a stable cloud environment

**2. GitHub Actions (Geometry Engine)**
Runs analyses:
  -Î¸ (theta)
  -Ï† (phi)
  -coherence
  -stress indexing
  -vector divergence

Every time new data is pushed, GitHub automatically recalculates all geometry outputs and commits them back to the repo.

This makes the framework fully automated and fully reproducible.

----------------------------------------------------------
**ğŸ“Š Motivation**

>VCF Research explores a simple but powerful idea:
>Markets and macroeconomies generate signals that behave like vectors â€” having magnitude, direction, and relationships that can align or conflict over time.
----------------------------------------------------------

**We study:**
  -periods of high coherence
  -rotational dynamics
  -macro â€œwave statesâ€
  -divergence between financial and economic signals
  -alignment patterns preceding regime shifts

The focus is structural understanding, not prediction.


----------------------------------------------------------

**ğŸ“ˆ Why GitHub + Colab?**
  -Cloud compute removes local hardware limitations
  -GitHub provides scientific reproducibility
  -Actions automate geometry updates
  -Easy versioning and academic traceability
  -Zero dependence on user hardware (phone, Chromebook, PC all work)



----------------------------------------------------------

**ğŸ”§ Requirements (automatically installed in Colab**
  -Python 3.10+
  -pandas
  -numpy
  -yfinance
  -pandas-datareader
  -scipy
  -matplotlib (optional)
  -GitHub personal access token (for repo push)

No local installation required.


----------------------------------------------------------

**ğŸ“ Workflow & How to Use**

**Running Notebooks:**
1. Open notebooks in Google Colab or Jupyter
2. Notebooks in `/notebooks` directory contain research and analysis code
3. For Colab integration with Google Drive:
   - Mount your Google Drive
   - Clone this repository or sync files
   - See `/docs/Colab_Zip_Builder.md` for detailed instructions

**Running Scripts:**
1. Scripts in `/scripts` handle data loading, normalization, and panel construction
2. Run locally with Python 3.10+ or in Colab environment
3. See individual script files for usage and parameters

**Data Organization:**
- **Raw data**: Place unprocessed CSV files in `/data_raw/`
- **Clean data**: Normalized and processed data outputs go to `/data_clean/`
- **Data sources**: Document all data sources in `/docs/DATA_SOURCES.md`
- **Updates**: When adding new data, update both the raw files and documentation

**Collaboration:**
- Use `/docs/log.md` to track experiments, findings, and process notes
- Document code changes clearly in commit messages
- Store reusable functions in `/src/` for use across notebooks and scripts
- Keep notebooks focused on specific analyses or workflows

For more detailed information, see documentation in `/docs/` directory.

----------------------------------------------------------

**ğŸ§ª Status**
-Phase I â€” Data Pipeline: âœ” Complete
-Phase II â€” Macro Panel: âœ” Complete
-Phase III â€” Geometry Engine: In Progress
-Phase IV â€” Academic Diagnostics / Publishable Metrics: Pending


----------------------------------------------------------

**ğŸ“œ License**
MIT (or whichever you choose)


----------------------------------------------------------
**ğŸ™Œ Maintainers**

Jason Rudder
ChatGPT (VCF Research Assistant)

----------------------------------------------------------

