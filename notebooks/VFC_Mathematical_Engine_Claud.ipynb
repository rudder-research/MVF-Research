{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOIkTUz03mSBDu63gfCw63+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rudder-research/VCF-RESEARCH/blob/main/VFC_Mathematical_Engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# VCF Research: Advanced Mathematical Models\n",
        "# Vector Coherence & Harmonic Variance Framework\n",
        "\n",
        "\"\"\"\n",
        "This module implements sophisticated mathematical models for:\n",
        "1. Vector variance decomposition\n",
        "2. Harmonic coherence analysis\n",
        "3. Dynamic mode analysis\n",
        "4. Multi-scale synchronization\n",
        "5. Geometric manifold dynamics\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.fft import fft, ifft, fftfreq\n",
        "from scipy.signal import hilbert, stft, istft\n",
        "from scipy.linalg import svd, eig\n",
        "from sklearn.decomposition import PCA\n",
        "from typing import Dict, Tuple, List\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# PART 1: VECTOR VARIANCE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "class VectorVarianceDecomposition:\n",
        "    \"\"\"\n",
        "    Decompose variance into geometric components:\n",
        "    - Directional variance (rotational)\n",
        "    - Magnitude variance (radial)\n",
        "    - Angular momentum\n",
        "    - Vector field divergence/curl\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, panel_df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        panel_df: DataFrame where each column is a time series\n",
        "        \"\"\"\n",
        "        self.panel = panel_df\n",
        "        self.n_series = panel_df.shape[1]\n",
        "        self.n_time = panel_df.shape[0]\n",
        "\n",
        "    def compute_vector_field(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Treat each time point as a vector in N-dimensional space\n",
        "        Returns: (n_time, n_series) array\n",
        "        \"\"\"\n",
        "        return self.panel.values\n",
        "\n",
        "    def magnitude_variance(self) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Variance in vector magnitude over time\n",
        "        ||v(t)||² variance\n",
        "        \"\"\"\n",
        "        vectors = self.compute_vector_field()\n",
        "        magnitudes = np.linalg.norm(vectors, axis=1)\n",
        "\n",
        "        return pd.Series({\n",
        "            'mean_magnitude': np.mean(magnitudes),\n",
        "            'std_magnitude': np.std(magnitudes),\n",
        "            'variance_magnitude': np.var(magnitudes),\n",
        "            'cv_magnitude': np.std(magnitudes) / np.mean(magnitudes)  # Coefficient of variation\n",
        "        })\n",
        "\n",
        "    def directional_variance(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Measure how much the direction changes (rotational variance)\n",
        "        Uses angular differences between consecutive vectors\n",
        "        \"\"\"\n",
        "        vectors = self.compute_vector_field()\n",
        "\n",
        "        # Normalize to unit vectors\n",
        "        norms = np.linalg.norm(vectors, axis=1, keepdims=True)\n",
        "        unit_vectors = vectors / (norms + 1e-10)\n",
        "\n",
        "        # Angular differences\n",
        "        angular_diffs = []\n",
        "        for i in range(len(unit_vectors) - 1):\n",
        "            cos_angle = np.dot(unit_vectors[i], unit_vectors[i+1])\n",
        "            cos_angle = np.clip(cos_angle, -1, 1)\n",
        "            angle = np.arccos(cos_angle)\n",
        "            angular_diffs.append(angle)\n",
        "\n",
        "        angular_diffs = np.array(angular_diffs)\n",
        "\n",
        "        return {\n",
        "            'mean_angular_change': np.mean(angular_diffs),\n",
        "            'std_angular_change': np.std(angular_diffs),\n",
        "            'total_rotation': np.sum(angular_diffs),\n",
        "            'rotation_rate': np.mean(angular_diffs)\n",
        "        }\n",
        "\n",
        "    def angular_momentum(self, window: int = 12) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Measure rotational momentum in vector space\n",
        "        L = r × v (cross product analog in high dimensions)\n",
        "        \"\"\"\n",
        "        vectors = self.compute_vector_field()\n",
        "        velocities = np.diff(vectors, axis=0)\n",
        "\n",
        "        # Rolling angular momentum\n",
        "        momentum = []\n",
        "        indices = []\n",
        "\n",
        "        for i in range(window, len(vectors)):\n",
        "            # Local rotation measure\n",
        "            local_vectors = vectors[i-window:i+1]\n",
        "            rotation = np.std([np.arctan2(v[1], v[0]) if len(v) >= 2 else 0\n",
        "                              for v in local_vectors])\n",
        "            momentum.append(rotation)\n",
        "            indices.append(self.panel.index[i])\n",
        "\n",
        "        return pd.Series(momentum, index=indices)\n",
        "\n",
        "    def vector_divergence(self, window: int = 12) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Measure if vectors are expanding (divergence > 0) or contracting (< 0)\n",
        "        Similar to div(F) in vector calculus\n",
        "        \"\"\"\n",
        "        vectors = self.compute_vector_field()\n",
        "\n",
        "        divergence = []\n",
        "        indices = []\n",
        "\n",
        "        for i in range(window, len(vectors)):\n",
        "            local = vectors[i-window:i+1]\n",
        "\n",
        "            # Measure expansion/contraction\n",
        "            center = np.mean(local, axis=0)\n",
        "            distances = [np.linalg.norm(v - center) for v in local]\n",
        "\n",
        "            # Positive divergence = expanding\n",
        "            div = (distances[-1] - distances[0]) / window\n",
        "            divergence.append(div)\n",
        "            indices.append(self.panel.index[i])\n",
        "\n",
        "        return pd.Series(divergence, index=indices)\n",
        "\n",
        "    def vector_curl(self, col_a: str, col_b: str) -> pd.Series:\n",
        "        \"\"\"\n",
        "        2D curl analog: rotation in the plane defined by two series\n",
        "        Measures circular motion in phase space\n",
        "        \"\"\"\n",
        "        x = self.panel[col_a].values\n",
        "        y = self.panel[col_b].values\n",
        "\n",
        "        dx = np.gradient(x)\n",
        "        dy = np.gradient(y)\n",
        "\n",
        "        # 2D curl = ∂y/∂x - ∂x/∂y (approximated)\n",
        "        curl = dy - dx\n",
        "\n",
        "        return pd.Series(curl, index=self.panel.index)\n",
        "\n",
        "    def explained_variance_by_dimension(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        PCA-style variance decomposition\n",
        "        Shows which 'eigendirections' explain most variance\n",
        "        \"\"\"\n",
        "        vectors = self.compute_vector_field()\n",
        "\n",
        "        # Center the data\n",
        "        centered = vectors - np.mean(vectors, axis=0)\n",
        "\n",
        "        # Covariance matrix\n",
        "        cov = np.cov(centered.T)\n",
        "\n",
        "        # Eigendecomposition\n",
        "        eigenvalues, eigenvectors = eig(cov)\n",
        "        eigenvalues = np.real(eigenvalues)\n",
        "\n",
        "        # Sort by explained variance\n",
        "        idx = eigenvalues.argsort()[::-1]\n",
        "        eigenvalues = eigenvalues[idx]\n",
        "\n",
        "        total_var = np.sum(eigenvalues)\n",
        "        explained_var = eigenvalues / total_var\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'eigenvalue': eigenvalues,\n",
        "            'explained_variance': explained_var,\n",
        "            'cumulative_variance': np.cumsum(explained_var)\n",
        "        })\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 2: HARMONIC COHERENCE MODELS\n",
        "# ============================================================================\n",
        "\n",
        "class HarmonicCoherence:\n",
        "    \"\"\"\n",
        "    Advanced harmonic analysis for coherence measurement\n",
        "    - Wavelet coherence (time-frequency)\n",
        "    - Cross-spectral density\n",
        "    - Phase-locking value\n",
        "    - Frequency-specific synchronization\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def wavelet_coherence(signal_a: np.ndarray, signal_b: np.ndarray,\n",
        "                         scales: np.ndarray = None) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Time-frequency coherence using continuous wavelet transform\n",
        "        Shows which frequencies are coherent at which times\n",
        "        \"\"\"\n",
        "        if scales is None:\n",
        "            scales = np.arange(1, min(128, len(signal_a)//4))\n",
        "\n",
        "        import pywt\n",
        "\n",
        "        # Continuous wavelet transform\n",
        "        coef_a, freqs_a = pywt.cwt(signal_a, scales, 'morl')\n",
        "        coef_b, freqs_b = pywt.cwt(signal_b, scales, 'morl')\n",
        "\n",
        "        # Cross-wavelet spectrum\n",
        "        cross_spectrum = coef_a * np.conj(coef_b)\n",
        "\n",
        "        # Wavelet coherence\n",
        "        coherence = np.abs(cross_spectrum)**2 / (np.abs(coef_a)**2 * np.abs(coef_b)**2 + 1e-10)\n",
        "\n",
        "        return coherence, scales\n",
        "\n",
        "    @staticmethod\n",
        "    def phase_locking_value(phase_a: np.ndarray, phase_b: np.ndarray,\n",
        "                           window: int = 50) -> pd.Series:\n",
        "        \"\"\"\n",
        "        PLV: Measures consistency of phase relationship over time\n",
        "        1 = perfect phase locking, 0 = random\n",
        "        \"\"\"\n",
        "        phase_diff = phase_b - phase_a\n",
        "\n",
        "        plv_series = []\n",
        "        for i in range(window, len(phase_diff)):\n",
        "            local_diff = phase_diff[i-window:i]\n",
        "            plv = np.abs(np.mean(np.exp(1j * local_diff)))\n",
        "            plv_series.append(plv)\n",
        "\n",
        "        return pd.Series(plv_series)  # No index - will be handled by caller\n",
        "\n",
        "    @staticmethod\n",
        "    def cross_spectral_density(signal_a: np.ndarray, signal_b: np.ndarray,\n",
        "                               fs: float = 12.0) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Frequency-domain correlation\n",
        "        Shows which frequencies are most coherent\n",
        "        \"\"\"\n",
        "        from scipy.signal import csd\n",
        "\n",
        "        f, Pxy = csd(signal_a, signal_b, fs=fs, nperseg=min(256, len(signal_a)//2))\n",
        "\n",
        "        return f, Pxy\n",
        "\n",
        "    @staticmethod\n",
        "    def instantaneous_coherence(signal_a: np.ndarray, signal_b: np.ndarray,\n",
        "                                window: int = 50) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Rolling coherence measure\n",
        "        Detects when signals move in/out of sync\n",
        "        \"\"\"\n",
        "        from scipy.signal import coherence\n",
        "\n",
        "        coh_series = []\n",
        "        for i in range(window, len(signal_a)):\n",
        "            local_a = signal_a[i-window:i]\n",
        "            local_b = signal_b[i-window:i]\n",
        "\n",
        "            f, Cxy = coherence(local_a, local_b, fs=1.0, nperseg=min(32, window//2))\n",
        "            coh_series.append(np.mean(Cxy))\n",
        "\n",
        "        return pd.Series(coh_series)  # No index - will be handled by caller\n",
        "\n",
        "    @staticmethod\n",
        "    def frequency_band_coherence(signal_a: np.ndarray, signal_b: np.ndarray,\n",
        "                                 bands: Dict[str, Tuple[float, float]] = None,\n",
        "                                 fs: float = 12.0) -> Dict:\n",
        "        \"\"\"\n",
        "        Coherence in specific frequency bands\n",
        "        bands: Dict of (low_freq, high_freq) in cycles per year\n",
        "        fs: Sampling frequency (12 for monthly data, 252 for daily)\n",
        "\n",
        "        Example for monthly data:\n",
        "        'business_cycle': (1/8, 1/1.5) = 8 years to 1.5 years periods\n",
        "        \"\"\"\n",
        "        from scipy.signal import butter, filtfilt, coherence\n",
        "\n",
        "        if bands is None:\n",
        "            # Default economic frequency bands (for monthly data)\n",
        "            # Frequencies in cycles per year, converted to normalized freq\n",
        "            bands = {\n",
        "                'high_freq': (0.5, 6.0),        # 2 months to 6 months periods\n",
        "                'business_cycle': (0.125, 0.67), # 1.5 to 8 years periods\n",
        "                'low_freq': (0.01, 0.125)        # > 8 years periods\n",
        "            }\n",
        "\n",
        "        results = {}\n",
        "        nyquist = fs / 2  # Nyquist frequency\n",
        "\n",
        "        for band_name, (low, high) in bands.items():\n",
        "            try:\n",
        "                # Normalize frequencies to [0, 1] where 1 = Nyquist\n",
        "                low_norm = low / nyquist\n",
        "                high_norm = high / nyquist\n",
        "\n",
        "                # Ensure valid range\n",
        "                low_norm = max(0.001, min(low_norm, 0.99))\n",
        "                high_norm = max(0.001, min(high_norm, 0.99))\n",
        "\n",
        "                # Make sure low < high\n",
        "                if low_norm >= high_norm:\n",
        "                    low_norm = high_norm * 0.5\n",
        "\n",
        "                # Bandpass filter\n",
        "                if low_norm <= 0.001:\n",
        "                    b, a = butter(2, high_norm, btype='low')\n",
        "                else:\n",
        "                    b, a = butter(2, [low_norm, high_norm], btype='band')\n",
        "\n",
        "                filtered_a = filtfilt(b, a, signal_a)\n",
        "                filtered_b = filtfilt(b, a, signal_b)\n",
        "\n",
        "                # Coherence in this band\n",
        "                f, Cxy = coherence(filtered_a, filtered_b, fs=fs)\n",
        "\n",
        "                results[band_name] = {\n",
        "                    'mean_coherence': np.mean(Cxy),\n",
        "                    'max_coherence': np.max(Cxy),\n",
        "                    'coherence_std': np.std(Cxy),\n",
        "                    'freq_range': (low, high)\n",
        "                }\n",
        "            except Exception as e:\n",
        "                results[band_name] = {\n",
        "                    'mean_coherence': np.nan,\n",
        "                    'max_coherence': np.nan,\n",
        "                    'coherence_std': np.nan,\n",
        "                    'error': str(e)\n",
        "                }\n",
        "\n",
        "        return results\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 3: DYNAMIC MODE DECOMPOSITION (DMD)\n",
        "# ============================================================================\n",
        "\n",
        "class DynamicModeDecomposition:\n",
        "    \"\"\"\n",
        "    DMD extracts spatiotemporal coherent structures\n",
        "    Discovers underlying dynamics from data\n",
        "\n",
        "    Perfect for finding:\n",
        "    - Oscillatory modes in macro data\n",
        "    - Growth/decay rates\n",
        "    - Dominant frequencies\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, panel_df: pd.DataFrame):\n",
        "        self.panel = panel_df\n",
        "        self.modes = None\n",
        "        self.eigenvalues = None\n",
        "        self.amplitudes = None\n",
        "\n",
        "    def compute_dmd(self, rank: int = None) -> Dict:\n",
        "        \"\"\"\n",
        "        Standard DMD algorithm\n",
        "        X' = A X  (find A via DMD)\n",
        "        \"\"\"\n",
        "        X = self.panel.values.T  # (n_series, n_time)\n",
        "\n",
        "        # Split into snapshots\n",
        "        X1 = X[:, :-1]\n",
        "        X2 = X[:, 1:]\n",
        "\n",
        "        # SVD of X1\n",
        "        U, s, Vt = svd(X1, full_matrices=False)\n",
        "\n",
        "        if rank:\n",
        "            U = U[:, :rank]\n",
        "            s = s[:rank]\n",
        "            Vt = Vt[:rank, :]\n",
        "\n",
        "        # DMD operator\n",
        "        S_inv = np.diag(1.0 / s)\n",
        "        A_tilde = U.T @ X2 @ Vt.T @ S_inv\n",
        "\n",
        "        # Eigendecomposition\n",
        "        eigenvalues, eigenvectors = eig(A_tilde)\n",
        "\n",
        "        # DMD modes\n",
        "        modes = X2 @ Vt.T @ S_inv @ eigenvectors\n",
        "\n",
        "        # Amplitudes\n",
        "        amplitudes = np.linalg.lstsq(modes, X[:, 0], rcond=None)[0]\n",
        "\n",
        "        self.modes = modes\n",
        "        self.eigenvalues = eigenvalues\n",
        "        self.amplitudes = amplitudes\n",
        "\n",
        "        return {\n",
        "            'modes': modes,\n",
        "            'eigenvalues': eigenvalues,\n",
        "            'amplitudes': amplitudes,\n",
        "            'frequencies': np.log(eigenvalues).imag / (2 * np.pi),\n",
        "            'growth_rates': np.log(np.abs(eigenvalues))\n",
        "        }\n",
        "\n",
        "    def reconstruct(self, mode_indices: List[int] = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Reconstruct time series using selected modes\n",
        "        \"\"\"\n",
        "        if self.modes is None:\n",
        "            raise ValueError(\"Must run compute_dmd() first\")\n",
        "\n",
        "        if mode_indices is None:\n",
        "            mode_indices = range(len(self.eigenvalues))\n",
        "\n",
        "        n_time = self.panel.shape[0]\n",
        "        time_dynamics = np.array([\n",
        "            self.amplitudes[i] * (self.eigenvalues[i] ** np.arange(n_time))\n",
        "            for i in mode_indices\n",
        "        ])\n",
        "\n",
        "        X_reconstructed = (self.modes[:, mode_indices] @ time_dynamics).real\n",
        "\n",
        "        return pd.DataFrame(\n",
        "            X_reconstructed.T,\n",
        "            columns=self.panel.columns,\n",
        "            index=self.panel.index\n",
        "        )\n",
        "\n",
        "    def dominant_modes(self, n_modes: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Find modes with largest amplitudes (most important dynamics)\n",
        "        \"\"\"\n",
        "        if self.modes is None:\n",
        "            raise ValueError(\"Must run compute_dmd() first\")\n",
        "\n",
        "        mode_power = np.abs(self.amplitudes)\n",
        "        top_indices = np.argsort(mode_power)[-n_modes:][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            freq = np.log(self.eigenvalues[idx]).imag / (2 * np.pi)\n",
        "            growth = np.log(np.abs(self.eigenvalues[idx]))\n",
        "\n",
        "            results.append({\n",
        "                'mode_index': idx,\n",
        "                'amplitude': np.abs(self.amplitudes[idx]),\n",
        "                'frequency': freq,\n",
        "                'period': 1.0 / freq if freq != 0 else np.inf,\n",
        "                'growth_rate': growth,\n",
        "                'stable': growth < 0\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 4: MULTI-SCALE COHERENCE\n",
        "# ============================================================================\n",
        "\n",
        "class MultiScaleCoherence:\n",
        "    \"\"\"\n",
        "    Analyze coherence across different time scales\n",
        "    Uses empirical mode decomposition (EMD)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def empirical_mode_decomposition(signal: np.ndarray, max_imf: int = 5) -> List[np.ndarray]:\n",
        "        \"\"\"\n",
        "        Decompose signal into intrinsic mode functions (IMFs)\n",
        "        Each IMF represents a different time scale\n",
        "        \"\"\"\n",
        "        from scipy.signal import hilbert\n",
        "\n",
        "        imfs = []\n",
        "        residual = signal.copy()\n",
        "\n",
        "        for _ in range(max_imf):\n",
        "            # Simple EMD implementation (sifting process)\n",
        "            imf = MultiScaleCoherence._sift(residual)\n",
        "\n",
        "            if imf is None:\n",
        "                break\n",
        "\n",
        "            imfs.append(imf)\n",
        "            residual = residual - imf\n",
        "\n",
        "            if np.std(residual) < 0.01 * np.std(signal):\n",
        "                break\n",
        "\n",
        "        imfs.append(residual)  # Trend\n",
        "        return imfs\n",
        "\n",
        "    @staticmethod\n",
        "    def _sift(signal: np.ndarray, max_iter: int = 10) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        EMD sifting iteration\n",
        "        \"\"\"\n",
        "        from scipy.interpolate import CubicSpline\n",
        "\n",
        "        h = signal.copy()\n",
        "\n",
        "        for _ in range(max_iter):\n",
        "            # Find extrema\n",
        "            peaks = []\n",
        "            troughs = []\n",
        "\n",
        "            for i in range(1, len(h) - 1):\n",
        "                if h[i] > h[i-1] and h[i] > h[i+1]:\n",
        "                    peaks.append(i)\n",
        "                elif h[i] < h[i-1] and h[i] < h[i+1]:\n",
        "                    troughs.append(i)\n",
        "\n",
        "            if len(peaks) < 3 or len(troughs) < 3:\n",
        "                return None\n",
        "\n",
        "            # Cubic spline envelopes\n",
        "            upper_env = CubicSpline(peaks, h[peaks], extrapolate=True)\n",
        "            lower_env = CubicSpline(troughs, h[troughs], extrapolate=True)\n",
        "\n",
        "            # Mean envelope\n",
        "            x = np.arange(len(h))\n",
        "            mean_env = (upper_env(x) + lower_env(x)) / 2\n",
        "\n",
        "            h_new = h - mean_env\n",
        "\n",
        "            # Check stopping criterion\n",
        "            if np.sum((h - h_new)**2) / np.sum(h**2) < 0.01:\n",
        "                return h_new\n",
        "\n",
        "            h = h_new\n",
        "\n",
        "        return h\n",
        "\n",
        "    @staticmethod\n",
        "    def scale_coherence(signal_a: np.ndarray, signal_b: np.ndarray,\n",
        "                       max_scales: int = 5) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Compute coherence at each time scale (IMF level)\n",
        "        \"\"\"\n",
        "        from scipy.signal import coherence\n",
        "\n",
        "        imfs_a = MultiScaleCoherence.empirical_mode_decomposition(signal_a, max_scales)\n",
        "        imfs_b = MultiScaleCoherence.empirical_mode_decomposition(signal_b, max_scales)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, (imf_a, imf_b) in enumerate(zip(imfs_a, imfs_b)):\n",
        "            # Ensure same length\n",
        "            min_len = min(len(imf_a), len(imf_b))\n",
        "            imf_a = imf_a[:min_len]\n",
        "            imf_b = imf_b[:min_len]\n",
        "\n",
        "            # Coherence for this scale\n",
        "            f, Cxy = coherence(imf_a, imf_b, fs=1.0)\n",
        "\n",
        "            # Typical period for this IMF\n",
        "            zero_crossings = np.where(np.diff(np.sign(imf_a)))[0]\n",
        "            period = 2 * np.mean(np.diff(zero_crossings)) if len(zero_crossings) > 1 else np.nan\n",
        "\n",
        "            results.append({\n",
        "                'scale': i + 1,\n",
        "                'coherence': np.mean(Cxy),\n",
        "                'max_coherence': np.max(Cxy),\n",
        "                'typical_period': period\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(results)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# PART 5: INTEGRATED ANALYSIS CLASS\n",
        "# ============================================================================\n",
        "\n",
        "class VCFMathEngine:\n",
        "    \"\"\"\n",
        "    Unified interface for all VCF mathematical models\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, panel_df: pd.DataFrame):\n",
        "        self.panel = panel_df\n",
        "        self.vector_variance = VectorVarianceDecomposition(panel_df)\n",
        "        self.dmd = DynamicModeDecomposition(panel_df)\n",
        "\n",
        "    def full_vector_analysis(self) -> Dict:\n",
        "        \"\"\"\n",
        "        Complete vector variance decomposition\n",
        "        \"\"\"\n",
        "        return {\n",
        "            'magnitude_variance': self.vector_variance.magnitude_variance(),\n",
        "            'directional_variance': self.vector_variance.directional_variance(),\n",
        "            'angular_momentum': self.vector_variance.angular_momentum(),\n",
        "            'divergence': self.vector_variance.vector_divergence(),\n",
        "            'explained_variance': self.vector_variance.explained_variance_by_dimension()\n",
        "        }\n",
        "\n",
        "    def full_harmonic_analysis(self, col_a: str, col_b: str, fs: float = 12.0) -> Dict:\n",
        "        \"\"\"\n",
        "        Complete harmonic coherence analysis for two series\n",
        "        fs: Sampling frequency (12 for monthly, 252 for daily, 1 for annual)\n",
        "        \"\"\"\n",
        "        signal_a = self.panel[col_a].values\n",
        "        signal_b = self.panel[col_b].values\n",
        "\n",
        "        # Phase angles\n",
        "        phase_a = np.angle(hilbert(signal_a))\n",
        "        phase_b = np.angle(hilbert(signal_b))\n",
        "\n",
        "        return {\n",
        "            'phase_locking': HarmonicCoherence.phase_locking_value(phase_a, phase_b),\n",
        "            'cross_spectrum': HarmonicCoherence.cross_spectral_density(signal_a, signal_b, fs=fs),\n",
        "            'instantaneous_coh': HarmonicCoherence.instantaneous_coherence(signal_a, signal_b),\n",
        "            'frequency_bands': HarmonicCoherence.frequency_band_coherence(signal_a, signal_b, fs=fs),\n",
        "            'multi_scale': MultiScaleCoherence.scale_coherence(signal_a, signal_b)\n",
        "        }\n",
        "\n",
        "    def dynamic_modes_analysis(self, n_modes: int = 5) -> Dict:\n",
        "        \"\"\"\n",
        "        DMD analysis to find dominant temporal patterns\n",
        "        \"\"\"\n",
        "        dmd_results = self.dmd.compute_dmd()\n",
        "        dominant = self.dmd.dominant_modes(n_modes)\n",
        "\n",
        "        return {\n",
        "            'dmd_results': dmd_results,\n",
        "            'dominant_modes': dominant,\n",
        "            'reconstruction': self.dmd.reconstruct(list(range(n_modes)))\n",
        "        }\n",
        "\n",
        "    def regime_detection(self, threshold: float = 0.5) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Detect regime changes based on coherence breaks\n",
        "        \"\"\"\n",
        "        # Compute pairwise coherence over time\n",
        "        cols = self.panel.columns\n",
        "        regime_signals = []\n",
        "\n",
        "        for i in range(len(cols)):\n",
        "            for j in range(i+1, len(cols)):\n",
        "                coh = HarmonicCoherence.instantaneous_coherence(\n",
        "                    self.panel[cols[i]].values,\n",
        "                    self.panel[cols[j]].values,\n",
        "                    window=50\n",
        "                )\n",
        "                regime_signals.append(coh.values)\n",
        "\n",
        "        # Average coherence across all pairs\n",
        "        avg_coherence = np.mean(regime_signals, axis=0)\n",
        "\n",
        "        # Regime changes = sharp drops in coherence\n",
        "        regime_changes = np.abs(np.diff(avg_coherence)) > threshold\n",
        "\n",
        "        # Create proper index\n",
        "        result_index = self.panel.index[50:]\n",
        "\n",
        "        return pd.DataFrame({\n",
        "            'coherence': avg_coherence,\n",
        "            'regime_change': np.concatenate([[False], regime_changes])\n",
        "        }, index=result_index)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "    Demonstration of all models\n",
        "    \"\"\"\n",
        "\n",
        "    # Create sample panel (replace with your data)\n",
        "    dates = pd.date_range('2010-01-01', periods=200, freq='M')\n",
        "    panel = pd.DataFrame({\n",
        "        'CPI': np.cumsum(np.random.randn(200) * 0.2) + 100,\n",
        "        'Yield_10Y': 3 + np.sin(np.linspace(0, 8*np.pi, 200)) + np.random.randn(200) * 0.3,\n",
        "        'XLF': np.cumsum(np.random.randn(200) * 2) + 50,\n",
        "        'XLE': np.cumsum(np.random.randn(200) * 2.5) + 45\n",
        "    }, index=dates)\n",
        "\n",
        "    # Initialize engine\n",
        "    engine = VCFMathEngine(panel)\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"VCF MATHEMATICAL ANALYSIS\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # 1. Vector Analysis\n",
        "    print(\"\\n1. VECTOR VARIANCE DECOMPOSITION\")\n",
        "    print(\"-\" * 70)\n",
        "    vector_results = engine.full_vector_analysis()\n",
        "    print(\"\\nMagnitude Variance:\")\n",
        "    print(vector_results['magnitude_variance'])\n",
        "    print(\"\\nDirectional Variance:\")\n",
        "    print(vector_results['directional_variance'])\n",
        "\n",
        "    # 2. Harmonic Analysis\n",
        "    print(\"\\n2. HARMONIC COHERENCE ANALYSIS (CPI vs Yield)\")\n",
        "    print(\"-\" * 70)\n",
        "    harmonic_results = engine.full_harmonic_analysis('CPI', 'Yield_10Y')\n",
        "    print(\"\\nFrequency Band Coherence:\")\n",
        "    print(harmonic_results['frequency_bands'])\n",
        "\n",
        "    # 3. DMD Analysis\n",
        "    print(\"\\n3. DYNAMIC MODE DECOMPOSITION\")\n",
        "    print(\"-\" * 70)\n",
        "    dmd_results = engine.dynamic_modes_analysis(n_modes=3)\n",
        "    print(\"\\nDominant Modes:\")\n",
        "    print(dmd_results['dominant_modes'])\n",
        "\n",
        "    # 4. Regime Detection\n",
        "    print(\"\\n4. REGIME DETECTION\")\n",
        "    print(\"-\" * 70)\n",
        "    regimes = engine.regime_detection(threshold=0.3)\n",
        "    n_regimes = regimes['regime_change'].sum()\n",
        "    print(f\"\\nDetected {n_regimes} regime changes\")\n",
        "    print(f\"Mean coherence: {regimes['coherence'].mean():.3f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"Analysis complete. All models ready for production use.\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDm7aPm09QgM",
        "outputId": "22117c44-9755-4709-d653-3b00e1db3d3b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "VCF MATHEMATICAL ANALYSIS\n",
            "======================================================================\n",
            "\n",
            "1. VECTOR VARIANCE DECOMPOSITION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Magnitude Variance:\n",
            "mean_magnitude        137.602477\n",
            "std_magnitude          14.884387\n",
            "variance_magnitude    221.544971\n",
            "cv_magnitude            0.108169\n",
            "dtype: float64\n",
            "\n",
            "Directional Variance:\n",
            "{'mean_angular_change': np.float64(0.018617278228321713), 'std_angular_change': np.float64(0.01046253862907799), 'total_rotation': np.float64(3.704838367436021), 'rotation_rate': np.float64(0.018617278228321713)}\n",
            "\n",
            "2. HARMONIC COHERENCE ANALYSIS (CPI vs Yield)\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Frequency Band Coherence:\n",
            "{'high_freq': {'mean_coherence': np.float64(1.0), 'max_coherence': np.float64(1.0000000000000007), 'coherence_std': np.float64(2.644398206269932e-16), 'freq_range': (0.5, 6.0)}, 'business_cycle': {'mean_coherence': np.float64(1.0), 'max_coherence': np.float64(1.0000000000000004), 'coherence_std': np.float64(2.407666597864512e-16), 'freq_range': (0.125, 0.67)}, 'low_freq': {'mean_coherence': np.float64(1.0), 'max_coherence': np.float64(1.0000000000000007), 'coherence_std': np.float64(2.343450582687896e-16), 'freq_range': (0.01, 0.125)}}\n",
            "\n",
            "3. DYNAMIC MODE DECOMPOSITION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Dominant Modes:\n",
            "   mode_index   amplitude  frequency      period  growth_rate  stable\n",
            "0           0  146.929878    0.00000         inf     0.000148   False\n",
            "1           1   97.278026    0.00139  719.652235    -0.025806    True\n",
            "2           2   97.278026   -0.00139 -719.652235    -0.025806    True\n",
            "\n",
            "4. REGIME DETECTION\n",
            "----------------------------------------------------------------------\n",
            "\n",
            "Detected 0 regime changes\n",
            "Mean coherence: 0.527\n",
            "\n",
            "======================================================================\n",
            "Analysis complete. All models ready for production use.\n",
            "======================================================================\n"
          ]
        }
      ]
    }
  ]
}
